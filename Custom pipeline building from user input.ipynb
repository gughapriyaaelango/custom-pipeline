{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af3a1b5",
   "metadata": {},
   "source": [
    "<h2>Building a custom pipeline</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f4de8",
   "metadata": {},
   "source": [
    "Imagining a user-friendly UI for users to build custom ETL pipelines using drag-and-drop method.<br> \n",
    "\n",
    "In Graph representation, the pipeline can be represented as a graph, with nodes as vertices and connections as edges. <br> \n",
    "\n",
    "Nodes can be categorized as Source/Target, Transform, Join/Split, Custom and Subgraph. Each node will have unique id and configuration details. The user defines the nodes and connects them to define the workflow. This is the example pipeline assumed for this task:<br>\n",
    "<b>Oracle CDC >> Data ingestion, transformation, denormalization >> Delta Lake </b><br>\n",
    "\n",
    "\n",
    "The pipeline can be represented as a JSON object containing an array of nodes, where each node represents a step in the ETL process.<br> \n",
    "\n",
    "To compile and execute:<br> \n",
    "The JSON representation is interpreted and the necessary Spark code is compiled to execute the ETL pipeline. The function of each node will be mapped from sql to Spark.<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d8139",
   "metadata": {},
   "source": [
    "Oracle CDC with JDBC connection can be used to ingest the data to staging layer or transform further to delta lake.<br> \n",
    "<b>Important config details -</b><br>\n",
    "Using source database authentication <br>\n",
    "Specifying source and target table, their primary keys and incremental column<br>\n",
    "Number of partitions/mappers, estimated record size<br>\n",
    "If needed for hive connection, hive authentication to query from presto<br>\n",
    "\n",
    "<b>Oracle CDC - Change data capture from last succcessful job time</b><br>\n",
    "From Oracle database, whatever insert,update,delete has changed from last successful job time, only the changes will be processed. This incremental processing of data requires us to select a specific column to capture changes like the insert_timestamp. The challenge can be a lag of 1 run when performing incremental joins.(can be more prone when using complex joins, window functions, etc.)Example:<br>\n",
    "1st run<br>\n",
    "Main. Reference<br>\n",
    "Address Zipcode  Join - addr_zip<br>\n",
    "1 60601 60601    60601 - 60601<br>\n",
    "2 60602 No Match 60602 - null<br>\n",
    "2nd Run<br>\n",
    "Address Zipcode Join - addr_zip<br>\n",
    "3 60603 60602   60601 - 60601<br>\n",
    "4 60604 60603   60602 - 60602<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f44c0",
   "metadata": {},
   "source": [
    "<b>Transformation logic assumed: </b><br>\n",
    "Task 1 - Explode table customer from array datatype to columns<br>\n",
    "Task 2 - Left join customer and orders table<br>\n",
    "Task 3 - Filter only columns where delete_ind = 0<br>\n",
    "Task 4 - Select only distinct records<br>\n",
    "Task 5 - Get counts of distinct orders of each customer<br>\n",
    "Task 6 - Aggregate by customer id and get max date for latest order<br>\n",
    "Task 7 - Window partition by customer id and get max bill sequence id<br>\n",
    "Task 8 - When order discount == \"BlackFriday\", True, False<br>\n",
    "Task 9 - Write True as 1 and False as 0<br>\n",
    "Task 10 - Take 10% off the order price <br>\n",
    "Task 11 - Create UDF to take customer id, order id, partition id and concatenate into UUID using |<br>\n",
    "Task 12 - Check if order price is null<br>\n",
    "Task 13 - Coalesce the order price<br>\n",
    "Task 14 - Create column order count to count number of orders for each unique customer<br>\n",
    "Task 15 - Broadcast a small table order category<br>\n",
    "Task 16 - Trim the order name column strings<br>\n",
    "Task 17 - Select purchase year and check if it is above 2018<br>\n",
    "Task 18 - Sort by the customer id<br>\n",
    "Task 19 - Rename column seq_id to sequence_id<br>\n",
    "Task 20 - Format and denormalise to write data to delta lake<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d0f4b",
   "metadata": {},
   "source": [
    "<h4>Giving the Pipeline Representation as JSON input:</h4><br>\n",
    "Each transform node contains an operation, which is later performed as spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bda3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"pipeline\": [\n",
    "    {\n",
    "      \"id\": \"customerSource\",\n",
    "      \"type\": \"jdbc\",\n",
    "      \"config\": {\n",
    "        \"connection\": {\n",
    "          \"url\": \"jdbc:oracle:thin:@//localhost:1521/ORCL\",\n",
    "          \"username\": \"your_username\",\n",
    "          \"password\": \"your_password\"\n",
    "        },\n",
    "        \"table\": \"customer\",\n",
    "        \"cdc\": {\n",
    "          \"enabled\": true,\n",
    "          \"incrementalColumn\": \"last_modified_ts\",\n",
    "          \"lastSuccessfulJobTime\": \"2023-11-25T00:00:00Z\",\n",
    "          \"primaryKey\": [\"customer_id\"]\n",
    "        },\n",
    "        \"partitions\": 5,\n",
    "        \"mappers\": 3\n",
    "      },\n",
    "      \"hive\": {\n",
    "        \"connection\": {\n",
    "          \"url\": \"jdbc:hive2://hive-server:10000/default\",\n",
    "          \"username\": \"hive_username\",\n",
    "          \"password\": \"hive_password\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"ordersSource\",\n",
    "      \"type\": \"jdbc\",\n",
    "      \"config\": {\n",
    "        \"connection\": {\n",
    "          \"url\": \"jdbc:oracle:thin:@//localhost:1521/ORCL\",\n",
    "          \"username\": \"your_username\",\n",
    "          \"password\": \"your_password\"\n",
    "        },\n",
    "        \"table\": \"orders\",\n",
    "        \"cdc\": {\n",
    "          \"enabled\": true,\n",
    "          \"incrementalColumn\": \"last_modified_ts\",\n",
    "          \"lastSuccessfulJobTime\": \"2023-11-25T00:00:00Z\",\n",
    "          \"primaryKey\": [\"order_id\"],\n",
    "          \"pollingInterval\": \"5000\"\n",
    "        },\n",
    "        \"partitions\": 5,\n",
    "        \"mappers\": 3\n",
    "      },\n",
    "      \"hive\": {\n",
    "        \"connection\": {\n",
    "          \"url\": \"jdbc:hive2://hive-server:10000/default\",\n",
    "          \"username\": \"hive_username\",\n",
    "          \"password\": \"hive_password\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"transformNode\",\n",
    "      \"type\": \"transform\",\n",
    "      \"config\": {\n",
    "        \"transformType\": \"Spark\",\n",
    "        \"script\": \"transform_code.py\",\n",
    "        \"tasks\": [\n",
    "          {\n",
    "            \"id\": \"task1\",\n",
    "            \"operation\": \"explode\",\n",
    "            \"table\": \"customer\",\n",
    "            \"arrayColumn\": \"customer_details\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task2\",\n",
    "            \"operation\": \"left_join\",\n",
    "            \"leftTable\": \"customer\",\n",
    "            \"rightTable\": \"orders\",\n",
    "            \"on\": \"customer.customer_id = orders.customer_id\",\n",
    "            \"joinType\": \"left\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task3\",\n",
    "            \"operation\": \"filter\",\n",
    "            \"condition\": \"delete_ind == 0\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task4\",\n",
    "            \"operation\": \"distinct\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task5\",\n",
    "            \"operation\": \"count\",\n",
    "            \"groupBy\": [\"customer_id\"],\n",
    "            \"alias\": \"distinct_orders_count\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task6\",\n",
    "            \"operation\": \"aggregate\",\n",
    "            \"groupBy\": [\"customer_id\"],\n",
    "            \"aggregations\": {\n",
    "              \"latest_order_date\": \"max(order_date)\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task7\",\n",
    "            \"operation\": \"window_partition\",\n",
    "            \"partitionBy\": [\"customer_id\"],\n",
    "            \"orderBy\": [\"bill_sequence_id\"],\n",
    "            \"alias\": \"max_bill_sequence_id\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task8\",\n",
    "            \"operation\": \"map\",\n",
    "            \"column\": \"order_discount\",\n",
    "            \"mapping\": {\n",
    "              \"BlackFriday\": true,\n",
    "              \"Other\": false\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task9\",\n",
    "            \"operation\": \"map\",\n",
    "            \"column\": \"BlackFriday\",\n",
    "            \"mapping\": {\n",
    "              \"true\": 1,\n",
    "              \"false\": 0\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task10\",\n",
    "            \"operation\": \"percent_off\",\n",
    "            \"column\": \"order_price\",\n",
    "            \"percentage\": 10\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task11\",\n",
    "            \"operation\": \"udf\",\n",
    "            \"name\": \"concatenate_uuid\",\n",
    "            \"columns\": [\"customer_id\", \"order_id\", \"partition_id\"],\n",
    "            \"alias\": \"uuid\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task12\",\n",
    "            \"operation\": \"check_null\",\n",
    "            \"column\": \"order_price\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task13\",\n",
    "            \"operation\": \"coalesce\",\n",
    "            \"columns\": [\"order_price\"]\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task14\",\n",
    "            \"operation\": \"count\",\n",
    "            \"groupBy\": [\"customer_id\"],\n",
    "            \"alias\": \"order_count\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task15\",\n",
    "            \"operation\": \"broadcast\",\n",
    "            \"table\": \"order_category\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task16\",\n",
    "            \"operation\": \"trim\",\n",
    "            \"column\": \"order_name\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task17\",\n",
    "            \"operation\": \"filter\",\n",
    "            \"condition\": \"purchase_year > 2018\"\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task18\",\n",
    "            \"operation\": \"sort\",\n",
    "            \"columns\": [\"customer_id\"]\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task19\",\n",
    "            \"operation\": \"rename\",\n",
    "            \"columns\": {\"seq_id\": \"sequence_id\"}\n",
    "          },\n",
    "          {\n",
    "            \"id\": \"task20\",\n",
    "            \"operation\": \"format_and_denormalize\"\n",
    "          }\n",
    "        ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"destinationNode\",\n",
    "      \"type\": \"destination\",\n",
    "      \"config\": {\n",
    "        \"destinationType\": \"deltaLake\",\n",
    "        \"path\": \"/path/to/delta/lake\",\n",
    "        \"cdc\": {\n",
    "          \"enabled\": true,\n",
    "          \"column\": \"last_modified\",\n",
    "          \"lastSuccessfulJobTime\": \"2023-11-25T00:00:00Z\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a0c65",
   "metadata": {},
   "source": [
    "<h4>Creating spark job from the JSON:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Function to perform explode operation - task 1\n",
    "def explode_task(df, column_name):\n",
    "    return df.selectExpr(\"*\", f\"explode({column_name}) as exploded_column\")\n",
    "\n",
    "# Function to perform left join operation - task 2\n",
    "def left_join_task(df_left, df_right, join_condition, join_type):\n",
    "    return df_left.join(df_right, join_condition, join_type)\n",
    "\n",
    "# Function to perform filter operation - task 3\n",
    "def filter_task(df, condition):\n",
    "    return df.filter(condition)\n",
    "\n",
    "# Function to perform distinct operation - task 4\n",
    "def distinct_task(df):\n",
    "    return df.distinct()\n",
    "\n",
    "# Function to perform count operation - task 5\n",
    "def count_task(df, group_by_columns, alias):\n",
    "    return df.groupBy(group_by_columns).agg(f.count().alias(alias))\n",
    "\n",
    "# Function to perform aggregate operation - task 6\n",
    "def aggregate_task(df, group_by_columns, aggregations):\n",
    "    return df.groupBy(group_by_columns).agg(aggregations)\n",
    "\n",
    "# Function to perform window partition operation - task 7\n",
    "def window_partition_task(df, partition_by_columns, order_by_columns, alias):\n",
    "    w = Window.partitionBy(partition_by_columns).orderBy(order_by_columns)\n",
    "    return df.withColumn(alias, f.first(df[order_by_columns]).over(w))\n",
    "\n",
    "# Function to perform map operation - task 8 & 9\n",
    "def map_task(df, column, mapping):\n",
    "    mapping_expr = f.create_map([f.lit(x) for x in sum(mapping.items(), ())])\n",
    "    return df.withColumn(column, mapping_expr[df[column]])\n",
    "\n",
    "# Function to perform percent off operation - task 10\n",
    "def percent_off_task(df, column, percentage):\n",
    "    return df.withColumn(column, df[column] * 0.9)\n",
    "\n",
    "# Function to perform UDF operation - task 11\n",
    "def udf_task(df, name, columns, alias):\n",
    "    concatenate_uuid_udf = f.udf(lambda *args: '|'.join(str(arg) for arg in args))\n",
    "    return df.withColumn(alias, concatenate_uuid_udf(*columns))\n",
    "\n",
    "# Function to perform check null operation - task 12\n",
    "def check_null_task(df, column):\n",
    "    return df.withColumn(column, f.when(f.col(column).isNull(), True).otherwise(False))\n",
    "\n",
    "# Function to perform coalesce operation - task 13\n",
    "def coalesce_task(df, columns):\n",
    "    return df.withColumn(columns[0], f.coalesce(*columns))\n",
    "\n",
    "# Function to perform count operation - task 14\n",
    "def count_task(df, group_by_columns, alias):\n",
    "    return df.groupBy(group_by_columns).agg(F.count().alias(alias))\n",
    "\n",
    "# Function to perform broadcast operation - task 15\n",
    "def broadcast_task(df, broadcast_table):\n",
    "    return df.join(f.broadcast(broadcast_table), \"order_id\", \"left\")\n",
    "\n",
    "# Function to perform trim operation - task 16\n",
    "def trim_task(df, column):\n",
    "    return df.withColumn(column, f.trim(df[column]))\n",
    "\n",
    "# Function to perform filter operation - task 17\n",
    "def filter_year_task(df, condition):\n",
    "    return df.filter(condition)\n",
    "\n",
    "# Function to perform sort operation - task 18\n",
    "def sort_task(df, columns):\n",
    "    return df.sort(*columns)\n",
    "\n",
    "# Function to perform rename operation - task 19\n",
    "def rename_task(df, columns):\n",
    "    for old_col, new_col in columns.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df\n",
    "\n",
    "# Function to perform format and denormalize operation - task 20\n",
    "def format_denormalize_task(df):\n",
    "    selected_columns = [\"customer_id\", \"latest_order_date\", \"max_bill_sequence_id\", \"BlackFriday\", \"order_price\", \"cust_uuid\",\n",
    "    \"order_count\", \"order_category\", \"order_name\", \"purchase_year\", \"sequence_id\"] \n",
    "    return df.select(*selected_columns)\n",
    "\n",
    "# Function to write to Delta Lake with incremental CDC\n",
    "def write_to_delta_lake(df, path, last_successful_time, cdc_column):\n",
    "    df.write.format(\"delta\").mode(\"append\").save(path)\n",
    "\n",
    "# Main function to execute the Spark job\n",
    "def execute_spark_job(pipeline_config):\n",
    "    spark = SparkSession.builder.appName(\"CustomETL\").getOrCreate()\n",
    "\n",
    "    for node in pipeline_config['pipeline']:\n",
    "        node_id = node['id']\n",
    "        node_type = node['type']\n",
    "        node_config = node['config']\n",
    "\n",
    "        if node_type == 'jdbc':\n",
    "            # Read data from JDBC source\n",
    "            df = spark.read.format('jdbc').option('url', node_config['connection']['url']) \\\n",
    "                .option('dbtable', node_config['table']).option('user', node_config['connection']['username']) \\\n",
    "                .option('password', node_config['connection']['password']).load()\n",
    "\n",
    "            # Apply CDC logic\n",
    "            if node_config['cdc']['enabled']:\n",
    "                df = df.filter(f.col(node_config['cdc']['incrementalColumn']) > node_config['cdc']['lastSuccessfulJobTime'])\n",
    "\n",
    "        elif node_type == 'transform':\n",
    "            # Read the script and tasks from the config\n",
    "            script_path = node_config['script']\n",
    "            tasks = node_config['tasks']\n",
    "\n",
    "            # Execute each transformation task\n",
    "            for task in tasks:\n",
    "                task_id = task['id']\n",
    "                operation = task['operation']\n",
    "\n",
    "                if operation == 'explode':\n",
    "                    df = explode_task(df, task['arrayColumn'])\n",
    "\n",
    "                elif operation == 'left_join':\n",
    "                    df_right = spark.read.format('jdbc').option('url', node_config['connection']['url']) \\\n",
    "                        .option('dbtable', task['rightTable']).option('user', node_config['connection']['username']) \\\n",
    "                        .option('password', node_config['connection']['password']).load()\n",
    "                    df = left_join_task(df, df_right, task['on'], task['joinType'])\n",
    "\n",
    "                elif operation == 'filter':\n",
    "                    df = filter_task(df, task['condition'])\n",
    "\n",
    "                elif operation == 'distinct':\n",
    "                    df = distinct_task(df)\n",
    "\n",
    "                elif operation == 'count':\n",
    "                    df = count_task(df, task['groupBy'], task['alias'])\n",
    "\n",
    "                elif operation == 'aggregate':\n",
    "                    aggregations = {col: task['aggregations'][col] for col in task['aggregations']}\n",
    "                    df = aggregate_task(df, task['groupBy'], aggregations)\n",
    "\n",
    "                elif operation == 'window_partition':\n",
    "                    df = window_partition_task(df, task['partitionBy'], task['orderBy'], task['alias'])\n",
    "\n",
    "                elif operation == 'map':\n",
    "                    df = map_task(df, task['column'], task['mapping'])\n",
    "\n",
    "                elif operation == 'percent_off':\n",
    "                    df = percent_off_task(df, task['column'], task['percentage'])\n",
    "\n",
    "                elif operation == 'udf':\n",
    "                    df = udf_task(df, task['name'], task['columns'], task['alias'])\n",
    "\n",
    "                elif operation == 'check_null':\n",
    "                    df = check_null_task(df, task['column'])\n",
    "\n",
    "                elif operation == 'coalesce':\n",
    "                    df = coalesce_task(df, task['columns'])\n",
    "\n",
    "                elif operation == 'count':\n",
    "                    df = count_task(df, task['groupBy'], task['alias'])\n",
    "\n",
    "                elif operation == 'broadcast':\n",
    "                    broadcast_table = spark.read.format('jdbc').option('url', node_config['connection']['url']) \\\n",
    "                        .option('dbtable', task['table']).option\n",
    "                    \n",
    "                elif operation == 'trim':\n",
    "                df = trim_task(df, task['column'])\n",
    "\n",
    "                elif operation == 'filter_year':\n",
    "                    df = filter_year_task(df, task['condition'])\n",
    "\n",
    "                elif operation == 'sort':\n",
    "                    df = sort_task(df, task['columns'])\n",
    "\n",
    "                elif operation == 'rename':\n",
    "                    df = rename_task(df, task['columns'])\n",
    "\n",
    "                elif operation == 'format_and_denormalize':\n",
    "                    df = format_denormalize_task(df)\n",
    "\n",
    "# Write to Delta Lake with incremental CDC\n",
    "destination_node = node for node in pipeline_config['pipeline'] if node['id'] == 'destinationNode', None\n",
    "if destination_node == 'destination' and destination_node['config']['destinationType'] == 'deltaLake':\n",
    "        delta_config = destination_node['config']['cdc']\n",
    "        write_to_delta_lake(df, delta_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c718e04",
   "metadata": {},
   "source": [
    "<h4>This will be the spark job without reading from JSON, if written manually:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ETLJob\").getOrCreate()\n",
    "\n",
    "# Read customer and orders tables from Parquet source using JDBC connection\n",
    "customer_df = spark.read.format(\"jdbc\").load(\"jdbc:oracle:thin:@//localhost:1521/ORCL\") \\\n",
    "    .option(\"dbtable\", \"(SELECT * FROM customer WHERE latest_modified_ts > '2023-11-25T00:00:00Z') as customer_cdc\") \\\n",
    "    .option(\"user\", \"your_username\").option(\"password\", \"your_password\").load()\n",
    "\n",
    "orders_df = spark.read.format(\"jdbc\").load(\"jdbc:oracle:thin:@//localhost:1521/ORCL\") \\\n",
    "    .option(\"dbtable\", \"(SELECT * FROM orders WHERE latest_modified_ts > '2023-11-25T00:00:00Z') as orders_cdc\") \\\n",
    "    .option(\"user\", \"your_username\").option(\"password\", \"your_password\").load()\n",
    "\n",
    "# Task 1 - Explode table customer from array datatype to columns\n",
    "customer_df = customer_df.withColumn(\"exploded_column\", f.explode(f.col(\"customer_details\")))\n",
    "\n",
    "# Task 2 - Left join customer and orders table\n",
    "joined_df = customer_df.join(orders_df, \"customer_id\", \"left\")\n",
    "\n",
    "# Task 3 - Filter only columns where delete_ind = 0\n",
    "filtered_df = joined_df.filter(f.col(\"delete_ind\") == 0)\n",
    "\n",
    "# Task 4 - Select only distinct records\n",
    "distinct_df = filtered_df.distinct()\n",
    "\n",
    "# Task 5 - Get counts of distinct orders of each customer\n",
    "distinct_orders_count_df = distinct_df.groupBy(\"customer_id\").agg(f.count(\"order_id\").alias(\"distinct_orders_count\"))\n",
    "\n",
    "# Task 6 - Aggregate by customer id and get max date for the latest order\n",
    "aggregated_df = joined_df.groupBy(\"customer_id\").agg(f.max(\"order_date\").alias(\"latest_order_date\"))\n",
    "\n",
    "# Task 7 - Window partition by customer id and get max bill sequence id\n",
    "w = Window.partitionBy(\"customer_id\").orderBy(f.col(\"bill_sequence_id\").desc())\n",
    "max_bill_sequence_id_df = filtered_df.withColumn(\"max_bill_sequence_id\", f.max(\"bill_sequence_id\").over(w))\n",
    "\n",
    "# Task 8 - When order discount == \"BlackFriday\", True, False\n",
    "df_with_true_false = distinct_df.withColumn(\"BlackFriday\", f.when(f.col(\"order_discount\") == \"BlackFriday\", True).otherwise(False))\n",
    "\n",
    "# Task 9 - Write True as 1 and False as 0\n",
    "mapped_df = df_with_true_false.withColumn(\"BlackFriday\", f.lit(1).when(f.col(\"BlackFriday\") == True, 0))\n",
    "\n",
    "# Task 10 - Take 10% off the order price\n",
    "discounted_df = mapped_df.withColumn(\"order_price\", f.col(\"order_price\") * 0.9)\n",
    "\n",
    "# Task 11 - Create UDF to take customer id, order id, partition id, and concatenate into UUID using |\n",
    "#UDF definition\n",
    "def concatenate_uuid(c_id, o_id, p_id):\n",
    "    return f\"{c_id}|{o_id}|{p_id}\"\n",
    "concatenate_uuid_udf = f.udf(concatenate_uuid, StringType())\n",
    "\n",
    "df_with_uuid = discounted_df.withColumn(\"cust_uuid\", concatenate_uuid_udf(f.col(\"customer_id\"), f.col(\"order_id\"), f.col(\"partition_id\")))\n",
    "\n",
    "# Task 12 - Check if order price is null\n",
    "df_checked_null = df_with_uuid.withColumn(\"order_price\", f.when(f.col(\"order_price\").isNull(), 0).otherwise(f.col(\"order_price\")))\n",
    "\n",
    "# Task 13 - Coalesce the order price\n",
    "df_coalesced = df_checked_null.withColumn(\"order_price\", f.coalesce(\"order_price\", 0))\n",
    "\n",
    "# Task 14 - Create column order count to count the number of orders for each unique customer\n",
    "w1 = Window.partitionBy(\"customer_id\")\n",
    "df_with_order_count = df_coalesced.withColumn(\"order_count\", f.count(\"order_id\").over(w1))\n",
    "\n",
    "# Task 15 - Broadcast a small table order category\n",
    "order_category = spark.read.format(\"parquet\").load(\"jdbc:oracle:thin:@//localhost:1521/ORCL\")\n",
    "df_with_broadcasted_category = df_with_order_count.join(f.broadcast(order_category), \"order_id\", \"left\")\n",
    "\n",
    "# Task 16 - Trim the order name column strings\n",
    "df_trimmed = df_with_broadcasted_category.withColumn(\"order_name\", f.trim(f.col(\"order_name\")))\n",
    "\n",
    "# Task 17 - Select purchase year and check if it is above 2018\n",
    "df_purchase_year_filtered = df_trimmed.withColumn(\"purchase_year\", f.col(\"purchase_date\").substr(1, 4).cast(\"int\")) \\\n",
    "    .filter(f.col(\"purchase_year\") > 2018)\n",
    "\n",
    "# Task 18 - Sort by the customer id\n",
    "sorted_df = df_purchase_year_filtered.orderBy(\"customer_id\")\n",
    "\n",
    "# Task 19 - Rename column seq_id to sequence_id\n",
    "df_renamed_sequence_id = sorted_df.withColumnRenamed(\"seq_id\", \"sequence_id\")\n",
    "\n",
    "# Task 20 - Format and denormalize to write data to delta lake\n",
    "df_denormalized = df_renamed_sequence_id.select(\n",
    "    \"customer_id\", \"latest_order_date\", \"max_bill_sequence_id\", \"BlackFriday\", \"order_price\", \"cust_uuid\",\n",
    "    \"order_count\", \"order_category\", \"order_name\", \"purchase_year\", \"sequence_id\"\n",
    ")\n",
    "\n",
    "df_denormalized.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"predicate\", f\"{cdc_column} > '{last_successful_job_time}'\").save(\"/path/to/delta/lake\")\n",
    "\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3580978",
   "metadata": {},
   "source": [
    "<h4>JSON example response generated from API:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fab916",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"status\": \"success\",\n",
    "  \"message\": \"Data retrieval successful\",\n",
    "  \"data\": {\n",
    "    \"customer\": [\n",
    "      {\n",
    "        \"customer_id\": 1,\n",
    "        \"name\": \"Gughapriyaa Elango\",\n",
    "        \"email\": \"gug@myemail.com\",\n",
    "        \"latest_modified_ts\": \"2023-11-26T08:00:00Z\"\n",
    "      },\n",
    "      {\n",
    "        \"customer_id\": 2,\n",
    "        \"name\": \"Priya Elango\",\n",
    "        \"email\": \"priya@someemail.com\",\n",
    "        \"latest_modified_ts\": \"2023-11-26T09:30:00Z\"\n",
    "      }, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
